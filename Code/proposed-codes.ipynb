{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12818489,"sourceType":"datasetVersion","datasetId":6885295},{"sourceId":12818807,"sourceType":"datasetVersion","datasetId":8106082}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Code for the Proposed Fixation Detection Algorithm","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import euclidean\nimport os\n\n# Load the image\nimage_path = 'scanpath_image.png'\n\n# Check if the image file exists\nif not os.path.isfile(image_path):\n    print(f\"Error: The file '{image_path}' does not exist.\")\nelse:\n    # Load the image\n    image = cv2.imread(image_path)\n\n    # Check if the image was loaded correctly\n    if image is None:\n        print(f\"Error: The image '{image_path}' could not be loaded.\")\n    else:\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Thresholding to create a binary image\n        _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY_INV)\n\n        # Find contours which correspond to gaze points\n        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Extract coordinates of gaze points\n        gaze_points = []\n        for contour in contours:\n            M = cv2.moments(contour)\n            if M['m00'] != 0:\n                cx = int(M['m10'] / M['m00'])\n                cy = int(M['m01'] / M['m00'])\n                gaze_points.append((cx, cy))\n\n        gaze_points = np.array(gaze_points)\n\n        # Plot the gaze points on the original image\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        plt.scatter(gaze_points[:, 0], gaze_points[:, 1], color='red')\n        plt.title('Detected Gaze Points')\n        plt.show()\n\n        # Define a threshold distance for fixation (in pixels)\n        fixation_threshold = 20\n\n        # Function to calculate fixations\n        def calculate_fixations(points, threshold):\n            fixations = []\n            current_fixation = [points[0]]\n\n            for point in points[1:]:\n                if euclidean(point, current_fixation[-1]) <= threshold:\n                    current_fixation.append(point)\n                else:\n                    fixations.append(np.mean(current_fixation, axis=0))\n                    current_fixation = [point]\n            \n            fixations.append(np.mean(current_fixation, axis=0))\n            return np.array(fixations)\n\n        # Calculate fixations\n        fixations = calculate_fixations(gaze_points, fixation_threshold)\n\n\n    \n        \n\n        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Code for the Proposed Character Selection Algorithm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the dataset\n        df = pd.read_csv('keyboard coordinates.csv')\n\n        # Function to check if a fixation is within AOI\n        def is_within_aoi(fixation, aoi):\n            x, y = fixation\n            return (aoi['X_MIN'] <= x <= aoi['X_MAX']) and (aoi['Y_MIN'] <= y <= aoi['Y_MAX'])\n\n        # Calculate fixation frequency for each character\n        df['fixation_frequency'] = 0\n\n        for fixation in fixations:\n            for index, row in df.iterrows():\n                if is_within_aoi(fixation, row):\n                    df.at[index, 'fixation_frequency'] += 1\n# Print fixation frequencies for each character\nprint(\"Fixation frequencies for each character:\")\nprint(df[['Character', 'fixation_frequency']])\n# Find the maximum fixation frequency\nmax_frequency = df['fixation_frequency'].max()\n\n# Filter characters with the maximum fixation frequency\nmax_frequency_chars = df[df['fixation_frequency'] == max_frequency]\n\n# Calculate average distance of fixations from the center of AOI for characters with maximum fixation frequency\nfor index, row in max_frequency_chars.iterrows():\n    fixations_in_aoi = [fixation for fixation in fixations if is_within_aoi(fixation, row)]\n    center_x = (row['X_MIN'] + row['X_MAX']) / 2\n    center_y = (row['Y_MIN'] + row['Y_MAX']) / 2\n    avg_distance = np.mean([np.sqrt((fix[0] - center_x) ** 2 + (fix[1] - center_y) ** 2) for fix in fixations_in_aoi])\n    max_frequency_chars.at[index, 'average_distance'] = avg_distance\n\n# Select the character with the minimum average distance\nfinal_character = max_frequency_chars.loc[max_frequency_chars['average_distance'].idxmin()]\n\n# Print characters with the maximum fixation frequency and their average distances\nprint(\"Characters with the highest fixation frequency and their average distances to fixations:\")\nprint(max_frequency_chars[['Character', 'fixation_frequency', 'average_distance']])\n\n# Print the final character\nprint(\"\\nFinal character:\")\nprint(final_character[['Character', 'fixation_frequency', 'average_distance']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Code for the Proposed Ensemble Model with Meta-Learner","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# Load dataset\nfile_path = 'normalized_dataset_finall.csv'\ndata = pd.read_csv(file_path)\nX = data.drop(columns=['Image name', 'Character'])\ny = data['Character']\n\n# Set desired augmented dataset size\ndesired_size = 1540\n\n# Enhanced Data Augmentation to achieve desired size\ndef augment_data_enhanced(X, y, desired_size):\n    augmented_X = X.copy()\n    augmented_y = y.copy()\n    \n    current_size = X.shape[0]\n\n    while augmented_X.shape[0] < desired_size:\n        # Add Gaussian noise and shifts\n        X_augmented = X + np.random.normal(0, 0.01, X.shape)\n        X_shifted = X + np.random.uniform(-2, 2, X.shape)\n        \n        # Scale data randomly\n        X_scaled = X * np.random.uniform(0.9, 1.1, X.shape)\n        \n        # Append augmented data to the original dataset\n        augmented_X = np.vstack([augmented_X, X_augmented, X_shifted, X_scaled])\n        augmented_y = np.hstack([augmented_y, y, y, y])\n        \n        # Trim if over the desired size\n        if augmented_X.shape[0] > desired_size:\n            extra_samples = augmented_X.shape[0] - desired_size\n            augmented_X = augmented_X[:-extra_samples]\n            augmented_y = augmented_y[:-extra_samples]\n\n    return augmented_X, augmented_y\n\n# Apply enhanced augmentation to the data\nX_augmented, y_augmented = augment_data_enhanced(X.values, y.values, desired_size)\n\n# Verify augmented dataset shape\nprint(f\"Augmented Dataset Shape: {X_augmented.shape}, Labels Shape: {y_augmented.shape}\")\n\n# Split augmented data\nX_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.2, random_state=42)\n\n# Custom ensemble with optimized meta-learner\nclass OptimizedEnsemble(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        # Base learners with initial parameter setups\n        self.rf = RandomForestClassifier(n_estimators=500, max_depth=25, min_samples_split=3, random_state=42)\n        self.svm = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=10, gamma='scale', random_state=42, probability=True))\n        self.nn = make_pipeline(StandardScaler(), MLPClassifier(hidden_layer_sizes=(200, 100), max_iter=500, random_state=42))\n        \n        # Using Logistic Regression as the meta-learner\n        self.meta_learner = LogisticRegression(max_iter=200, random_state=42)\n\n    def fit(self, X, y):\n        # Train base models\n        self.rf.fit(X, y)\n        self.svm.fit(X, y)\n        self.nn.fit(X, y)\n\n        # Stack predictions as meta-features\n        rf_pred = self.rf.predict_proba(X)\n        svm_pred = self.svm.predict_proba(X)\n        nn_pred = self.nn.predict_proba(X)\n        meta_features = np.hstack([rf_pred, svm_pred, nn_pred])\n        \n        # Train the meta-learner on stacked predictions\n        self.meta_learner.fit(meta_features, y)\n        return self\n\n    def predict(self, X):\n        # Generate predictions from each model\n        rf_pred = self.rf.predict_proba(X)\n        svm_pred = self.svm.predict_proba(X)\n        nn_pred = self.nn.predict_proba(X)\n\n        # Stack predictions and use meta-learner to predict final output\n        meta_features = np.hstack([rf_pred, svm_pred, nn_pred])\n        return self.meta_learner.predict(meta_features)\n\n    def predict_proba(self, X):\n        rf_pred = self.rf.predict_proba(X)\n        svm_pred = self.svm.predict_proba(X)\n        nn_pred = self.nn.predict_proba(X)\n        meta_features = np.hstack([rf_pred, svm_pred, nn_pred])\n        return self.meta_learner.predict_proba(meta_features)\n\n# Train the optimized ensemble model\noptimized_ensemble_model = OptimizedEnsemble()\noptimized_ensemble_model.fit(X_train, y_train)\n\n# Make predictions and evaluate the model\ny_pred = optimized_ensemble_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Optimized Ensemble Model Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}